{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e69f1c-9f74-4ded-a151-cb4964c451a1",
   "metadata": {},
   "source": [
    "# Machine Learning Vademecum\n",
    "## Introduzione\n",
    "Il machine learning (ML) è la disciplina il cui obiettivo è **migliorare le prestazioni di un sistema apprendendo dall'esperienza tramite metodi computazionali**.  \n",
    "\n",
    "Non è poi così diverso da quello che anche noi umani facciamo quotidianamente: di continuo ci basiamo su esperienze pregresse per fare predizioni più o meno accurate sulla realtà. \n",
    "Ad esempio, sappiamo prevedere con una certa sicurezza se una pesca sia buona o meno, semplicemente guardandola e toccandola, ancor prima di mangiarla.  \n",
    "La predizione non deriva da un ragionamento logico comprovato (altrimenti sarebbe sempre esatta), ma dall'essersi scontrati già più volte con le istanze del problema. \n",
    "Questo ci ha fornito, tramite quello che è un **processo induttivo**, un meccanismo risolutivo col quale affrontare istanze nuove, mai incontrate prima.\n",
    "\n",
    "Seguendo questo paradigma, l'idea alla base del ML è dare in pasto dei **dati** ad un cosiddetto **algoritomo di apprendimento**, che li usi per produrre un **modello** in grado di fornire risposte sufficientemente accurate di fronte a nuove istanze del problema.  \n",
    "Esattamente come le nostre predizioni possono rivelarsi errate, così possono esserlo le risposte fornite dal modello, al quale si associano quindi delle **metriche** per valutarne le performance.  \n",
    "\n",
    "Ma perché accontentarsi di una risposta *probabilmente* corretta al posto di una la cui correttezza è dimostrabile logicamente?\n",
    "\n",
    "### Due scenari\n",
    "#### 1. Il problema della nonna\n",
    "Esistono problemi per i quali **non siamo in grado di formulare una soluzione**, anche perché può non esistere.  \n",
    "Per una persona, stabilire se una foto ritragga o meno sua nonna è un compito triviale; per un computer non lo è affatto. Questo è un esempio di problema \"_non risolvibile_\", per cui ricorriamo al ML.\n",
    "\n",
    "#### 2. Il problema dello zaino\n",
    "Per alcuni problemi, formulare una soluzione è del tutto fattibile (se non facile), ma questa ha un **costo computazionale proibitivo**.  \n",
    "Nel <a href=\"https://it.wikipedia.org/wiki/Problema_dello_zaino\">problema dello zaino</a>, la soluzione _brute force_ è concettualmente semplice: basta considerare tutte le possibili combinazioni di oggetti e scegliere quella con il valore massimo senza superare il peso limite. Tuttavia, questa strategia diventa impraticabile al crescere del numero di oggetti ($n$): ad esempio, con $n=50$, si arriva a oltre un milione di miliardi di combinazioni da esaminare ($2^{50}$) .\n",
    "\n",
    "In queste due situazioni ricorriamo ad un modello di ML, nell'ottica che le sue risposte approssimino bene quelle esatte, se il problema ne prevede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7443356-0d79-490e-821c-06ae2f0e06b2",
   "metadata": {},
   "source": [
    "## Modalità di apprendimento\n",
    "In base a come si presentano i dati che l’algoritmo di apprendimento usa per produrre il modello, distinguiamo tra apprendimento **supervisionato** e non.\n",
    "\n",
    "### Apprendimento supervisionato\n",
    "Ogni osservazione è rappresentata da una coppia $(x, y)$, dove $x$ è l'istanza del problema e $y$ è la relativa soluzione (o etichetta).  \n",
    "Ad esempio, se $x$ è una foto della nonna da classificare, $y$ sarà la risposta _sì/no_.  \n",
    "Il dataset è quindi un insieme di coppie _istanza-soluzione_: $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}$.  \n",
    "Chiaramente deve esistere una relazione $f$ che leghi istanze e soluzioni, tale che $f(x) = y$. L'obiettivo del modello è proprio approssimare $f$, che prende il nome di _ground truth_. Le predizioni del modello vengono indicate come $\\hat{y}$.\n",
    "\n",
    "### Apprendimento non supervisionato\n",
    "Ogni osservazione è costituita esclusivamente dall'istanza del problema: $(x)$.  \n",
    "Un esempio di apprendimento non supervisionato è il _clustering_, in cui il modello deve raggruppare i dati in insiemi (_cluster_) basati su somiglianze o caratteristiche comuni. L'obiettivo è che le osservazioni all'interno di ciascun gruppo siano più simili tra loro rispetto a quelle di altri gruppi, permettendo di individuare pattern e strutture nascoste nei dati senza l'utilizzo delle etichette.\n",
    "\n",
    "**_Nota_**: il notebook si concentra sul ML supervisionato."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dbb0c48-04ce-448a-b934-6c030850c72e",
   "metadata": {},
   "source": [
    "## Valutare un modello\n",
    "Poiché l'obiettivo è approssimare la relazione di base (_ground truth_) tra istanze e soluzioni, è necessario **valutare la bontà di tale approssimazione**, ossia valutare il modello.  \n",
    "L'idea di base è molto semplice: si forniscono al modello delle osservazioni (coppie $(x, y)$), e per ciascuna si confronta la risposta prodotta dal modello, $\\hat{y}$, con la soluzione reale, $y$.  \n",
    "A partire da questo confronto, è possibile monitorare diverse **metriche** di valutazione delle prestazioni.\n",
    "\n",
    "### Metriche\n",
    "Ci sono diverse metriche, l'importante è utilizzare quella più opportuna per un determinato problema. <br>\n",
    "Ne definiamo quattro($D$ indica il dataset)\n",
    "- **Accuratezza**: indica il numero di elmenti del campione che sono stati classificati in modo corretto, rispetto al totale : $acc(f;D)= \\frac{\\sum_{i=1}^{m}I(f(x_i)=y_i)}{m}$;\n",
    "- **Errore**: indica il numero di elmenti del campione che sono stati classificati in modo errato, rispetto al totale : $E(f;D)=\\frac{\\sum_{i=1}^{m}I(f(x_i)\\neq y_i)}{m}$, ($E(f;D)=1-acc(f;D)$);\n",
    "- **Precisione**: indica quanti dei campioni previsti come positivi sono effettivamente positivi: $\\frac{Vero\\, positivi}{Vero\\, positivi + Falsi\\, positivi}$;\n",
    "- **Richiamo**: misura la capacità del modello di trovare tutti i campioni positivi: $\\frac{Vero\\, positivi}{Vero\\, positivi + Falsi\\, negativi}$.\n",
    "\n",
    "### Con quali dati?\n",
    "Una domanda che ci dobbiamo porre è: _\"Quali dati usiamo per valutare le performance del modello?\"_\n",
    "Intuitivamente vorremmo utilizzare gli stessi dati che abbiamo usato nella fase di allenamento(gli stessi che appartengono al **train set**).\n",
    "Vediamo, però, perché non è una buona idea:<br>\n",
    "Prendiamo come esempio un semplice problema di regressione, il cui obiettivo è trovare una curva che descriva al meglio l'andamento dei dati contenuti nel train set; in questo caso il modello non è altro che una funzione matematica e lo valuteremo in base a quanto si discosta mediamente dai \"punti\" del dataset.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/regressione.jpeg\" alt=\"Descrizione\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "I punti neri rappresentano il set di addestramento, mentre i punti bianchi corrispondono a dati nuovi, mai visti dal modello. L'ideale sarebbe avere un modello come A, che riesce a interpolare perfettamente tutti i punti, sia quelli del set di addestramento sia quelli nuovi. Al contrario, il modello B, pur mostrando ottime performance sul set di addestramento, si discosta notevolmente dai dati non visti, rendendolo un modello inefficace. Questo fenomeno è noto come overfitting: il modello si è adattato eccessivamente ai dati di addestramento, senza cogliere le relazioni sottostanti e quindi non è in grado di generalizzare su dati mai incontrati.\n",
    "\n",
    "Valutare un modello solo sul set di addestramento è simile a giudicare un allievo esclusivamente in base agli esercizi svolti in aula; ciò non fornisce indicazioni sul reale apprendimento dei concetti, ma solo sulla memorizzazione delle soluzioni già affrontate. Pertanto, è fondamentale testare il modello su un test set, cioè un insieme di dati che non è stato utilizzato durante l'allenamento.\n",
    "\n",
    "Tuttavia, valutare un modello sul set di addestramento non è del tutto inutile. Se un modello presenta performance scadenti su questo set, ciò può indicare che non è sufficientemente potente o espressivo per il problema in questione (un fenomeno noto come underfitting), offrendo così informazioni preziose. È importante, dunque, comprendere che buone performance sul set di addestramento non garantiscono la qualità del modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce266ef-68f6-4c6b-9617-8dba860f7b00",
   "metadata": {},
   "source": [
    "## Due tipologie di modello\n",
    "Presentiamo molto brevemente due tra le numerose tipologie di modello di ML, anche per illustrare più facilmente alcuni concetti più avanti.\n",
    "\n",
    "### Albero di decisione\n",
    "Si tratta di un albero in cui ogni nodo interno rappresenta una decisione basata su una caratteristica (o attributo) dell'esempio, e le foglie rappresentano le classi o le previsioni finali.  \n",
    "(fissata un’altezza massima) L’algoritmo di apprendimento produce, partendo dai dati, un albero di decisione; deve quindi stabilirne la struttura, le condizioni presenti nei nodi interni e le classi nelle foglie.  \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/decision_tree.png\" alt=\"Descrizione\" width=\"400\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "### KNN\n",
    "Gli esempi vengono disposti in uno spazio avente una dimensione per ogni possibile caratteristica/attributo delle istanze. Dunque ogni esempio corrisponde ad un punto in questo spazio.  \n",
    "Per classificare una nuova istanza, rappresentata da un nuovo punto, KNN calcola la distanza tra quest'ultimo e tutti i punti del dataset (solitamente utilizzando la distanza euclidea).\n",
    "Vengono poi selezionati i $k$ punti più vicini (da qui \"$k$-nearest neighbors\").\n",
    "La classe del nuovo punto viene determinata dalla classe più frequente tra i $k$ vicini selezionati nel caso della classificazione, oppure dalla media dei valori nel caso della regressione.  \n",
    "Nel caso di KNN non c’è una vera e propria fase di allenamento (vedremo poi nel paragrafo dedicato a parametri e iperparametri); semplicemente i dati vengono memorizzati.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/knn.png\" alt=\"Descrizione\" width=\"300\" height=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235e47a-763d-4838-9ad8-62950f504cfe",
   "metadata": {},
   "source": [
    "## Parametri e iperparametri\n",
    "I parametri sono le caratteristiche del modello che vengono definite durante la fase di apprendimento. Gli iperparametri, invece, sono le caratteristiche del modello che devono essere fissate *prima* della fase di apprendimento.\n",
    "\n",
    "Per gli alberi di decisione, ha senso stabilire come prima cosa un’altezza massima, che costituisce quindi un iperparametro. Fissata quella, l’allenamento stabilisce la topologia, le decisioni presenti nei nodi interni e le classi delle foglie; che sono i parametri. Lo scopo dell'allenamento è trovare i valori ottimali per i parametri del modello.  \n",
    "Prima dell'allenamento, i parametri hanno valori di default o addirittura casuali; al termine dell'allenamento, i parametri hanno i valori ottimali, ossia quelli che consentono al modello di compiere predizioni accurate.\n",
    "\n",
    "Nel caso di KNN, non c’è una vera e propria fase di apprendimento; di fatto non ci sono parametri da stabilire. C’è solo un iperparametro, ossia $k$.\n",
    "\n",
    "Ma se l’algoritmo di apprendimento trova i parametri, come si trovano gli iperparametri?\n",
    "\n",
    "### Trovare gli iperparametri\n",
    "In questa sezione prendiamo come riferimento KNN, che ha un solo iperparametro: $k$. Sappiamo che non c’è un vero e proprio allenamento, ma lo includeremo tra le fasi, generalizzando (si può pensare ad una fase di “collocazione dei dati nello spazio”).\n",
    "\n",
    "Non c’è un modo particolarmente furbo di trovare il valore ottimale di $k$. La cosa migliore da fare è anche la più intuitiva: stabilire un insieme di $n$ possibili valori per $k$ e lanciare un algoritmo di apprendimento per ciascuno di questi valori, generando così $n$ modelli allenati sullo stesso train set $S$. Ciascun modello viene poi valutato e si sceglie quello con la performance migliore.  \n",
    "Come già discusso (paragrafo “valutare un modello”), non possiamo misurare le performance limitandoci a $S$. Dunque, per scegliere il modello migliore tra gli $n$ generati, ricorriamo ad un cosiddetto **validation set** $V$, disgiunto da $S$. Denotiamo come $k^*$ l'iperparametro del modello che performa meglio.  \n",
    "Viene poi generato un nuovo modello $m$ avente $k = k^*$, ma allenandolo sull'unione $S \\cup V$. Chiaramente più dati si danno in pasto al modello, meglio performerà.  \n",
    "Infine, si utilizza un apposito test set $T$ per valutare le performance di $m$. Se sono soddisfacenti, si fa un altro “giro” di allenamento, questa volta su $S \\cup V \\cup T$.\n",
    "\n",
    "Se si hanno due o più iperparametri, si procede analogamente, testando ogni loro possibile combinazione e scegliendo quella che dà luogo alla performance migliore su $V$. Chiaramente le risorse computazionali richieste aumentano non poco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c0bca",
   "metadata": {},
   "source": [
    "## Addestramento e Valutazione dei Modelli\n",
    "\n",
    "Dato il dataset $ D = \\{(x_1, y_1), \\dots, (x_n, y_n)\\} $, come possiamo fare sia l'addestramento per la creazione del modello che il testing, cercando di ridurre al minimo l'overfitting?\n",
    "\n",
    "Una soluzione è quella di creare due sottoinsiemi del dataset $ D $, il Training Set $ T $ e il Test Set $ S $. Possiamo farlo in diversi modi, ad esempio utilizzando la metodologia $ \\boldsymbol{Hold \\ Out} $ oppure $ \\boldsymbol{Cross \\ Validation} $.\n",
    "\n",
    "### Hold Out\n",
    "\n",
    "La metodologia Hold Out consiste nel suddividere l'intero dataset $ D $ in due dataset, $ T $ (Training Set) e $ S $ (Test Set), in modo tale che $ D = T \\cup S $ e $ T \\cap S = \\emptyset $.\n",
    "\n",
    "Quindi, utilizzeremo il training set per creare il modello e poi testeremo il modello con il test set, in modo da poter calcolare un valore di accuratezza basato su quante volte risponde correttamente.\n",
    "\n",
    "In che modo deve essere diviso il dataset iniziale?\n",
    "Generalmente si cerca di avere una maggiore quantità di dati nel training set rispetto al test set. Una pratica comune è usare circa $ \\frac{2}{3} $ o $ \\frac{4}{5} $ degli esempi per l'addestramento e il resto per il test.\n",
    "\n",
    "Inoltre, entrambi gli insiemi devono essere rappresentativi e rispettare le proporzioni delle etichette nel dataset. Ad esempio, se nel dataset abbiamo l'80% delle foto con la nonna e il 20% senza, allora sia $ T $ che $ S $ dovrebbero mantenere l'80% di foto con la nonna e il 20% senza. Questo si chiama campionamento stratificato.\n",
    "\n",
    "Un singolo tentativo di testing con il metodo Hold Out di solito porta a una stima dell'errore poco affidabile. In pratica, spesso eseguiamo il metodo Hold Out più volte, suddividendo i dati in modo casuale a ogni prova, e utilizziamo la media degli errori come stima finale.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/holdout.webp\" alt=\"Descrizione\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "La metodologia Cross Validation consiste nel suddividere l'intero dataset $ D $ in $ k $ sottoinsiemi in modo tale che $ D = \\bigcup_{i=0}^{k} D_i $ e $ \\forall i, j \\in \\{0, 1, \\dots, k\\}, D_i \\cap D_j = \\emptyset $, con $ i \\neq j $.\n",
    "\n",
    "Ogni sottoinsieme cerca di mantenere la distribuzione originale dei dati tramite il campionamento stratificato. Quindi, verranno fatte $ k $ iterazioni, in cui, a ogni iterazione, un singolo insieme $ D_i $, che non è stato ancora utilizzato come test set, viene scelto come test set, mentre il training set viene composto dall'unione di tutti gli altri sottoinsiemi. A questo punto, possiamo ottenere la valutazione del modello.\n",
    "\n",
    "Finite le iterazioni, ci ritroveremo con $ k $ valutazioni, da cui otterremo un'unica valutazione complessiva calcolando la media.\n",
    "\n",
    "Poiché la stabilità e l'accuratezza della cross-validation dipendono in gran parte dal valore di $ k $, essa è anche conosciuta come k-fold cross-validation. Il valore di $ k $ più comunemente utilizzato è 10, e il metodo corrispondente è chiamato 10-fold cross-validation.\n",
    "\n",
    "Come nel metodo Hold-Out, ci sono diversi modi per suddividere il dataset $ D $ in $ k $ sottoinsiemi. Per ridurre l'errore introdotto dalla suddivisione, spesso ripetiamo la suddivisione casuale $ p $ volte e facciamo la media dei risultati delle $ p $ iterazioni di k-fold cross-validation. Un caso comune è la 10-times 10-fold cross-validation.\n",
    "\n",
    "Un caso speciale è il Leave-One-Out (LOO), dove abbiamo un dataset con cardinalità $ m $ e impostiamo $ k = m $. Le valutazioni sono molto accurate, ma il costo computazionale dell'addestramento di $ m $ modelli può essere proibitivo per dataset di grandi dimensioni.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/crossvalidation.png\" alt=\"Descrizione\" width=\"500\" height=\"500\">\n",
    "</div>\n",
    "\n",
    "### Tuning degli iperparametri\n",
    "\n",
    "Fino ad ora non abbiamo considerato gli iperparametri. Il nostro obiettivo è trovare gli iperparametri che producono il miglior risultato nel modello finale.\n",
    "\n",
    "Per capire quale iperparametro dia il miglior risultato, dobbiamo andare per tentativi.\n",
    "\n",
    "Dobbiamo considerare che gli iperparametri spesso sono valori reali, quindi è impossibile testarli tutti. Tipicamente, si utilizza un intervallo e una dimensione del passo per ciascun parametro, rendendo la ricerca meno esaustiva ma più gestibile.\n",
    "\n",
    "Nonostante questi accorgimenti, il tuning degli iperparametri rimane impegnativo. Se un algoritmo ha 3 iperparametri, e ciascuno considera 5 valori candidati, allora dovremmo valutare $ 5^3 = 125 $ combinazioni di parametri.\n",
    "\n",
    "Inoltre, abbiamo bisogno di dati separati per scegliere i migliori iperparametri. Non possiamo utilizzare né i dati del training set né quelli del test set, quindi suddividiamo ulteriormente il training set in un training set ridotto e un validation set $ V $.\n",
    "\n",
    "Possiamo suddividere i dati in modi diversi. Per esempio, se utilizziamo una suddivisione esterna (da dataset a training set e test set) con un metodo già visto, possiamo suddividere internamente (da training set a training set ridotto e validation set) con la stessa metodologia o con una diversa.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/tuning.png\" alt=\"Descrizione\" width=\"400\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "Tuttavia, attenzione nel caso si scelga di utilizzare la cross-validation per la divisione esterna: a ogni iterazione verrà utilizzato un modello diverso per il test, e non è detto che tutti i modelli di ogni iterazione usino gli stessi iperparametri. In questo caso, possiamo scegliere uno qualsiasi dei modelli addestrati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5cffbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
