{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e69f1c-9f74-4ded-a151-cb4964c451a1",
   "metadata": {},
   "source": [
    "# Machine Learning Vademecum\n",
    "## Introduzione\n",
    "Il machine learning (ML) è la disciplina il cui obiettivo è **migliorare le prestazioni di un sistema apprendendo dall'esperienza tramite metodi computazionali**.  \n",
    "\n",
    "Non è poi così diverso da quello che anche noi umani facciamo quotidianamente: di continuo ci basiamo su esperienze pregresse per fare predizioni più o meno accurate sulla realtà. \n",
    "Ad esempio, sappiamo prevedere con una certa sicurezza se una pesca sia buona o meno, semplicemente guardandola e toccandola, ancor prima di mangiarla.  \n",
    "La predizione non deriva da un ragionamento logico comprovato (altrimenti sarebbe sempre esatta), ma dall'essersi scontrati già più volte con le istanze del problema. \n",
    "Questo ci ha fornito, tramite quello che è un **processo induttivo**, un meccanismo risolutivo col quale affrontare istanze nuove, mai incontrate prima.\n",
    "\n",
    "Seguendo questo paradigma, l'idea alla base del ML è dare in pasto dei **dati** ad un cosiddetto **algoritomo di apprendimento**, che li usi per produrre un **modello** in grado di fornire risposte sufficientemente accurate di fronte a nuove istanze del problema.  \n",
    "Esattamente come le nostre predizioni possono rivelarsi errate, così possono esserlo le risposte fornite dal modello, al quale si associano quindi delle **metriche** per valutarne le performance.  \n",
    "\n",
    "Ma perché accontentarsi di una risposta *probabilmente* corretta al posto di una la cui correttezza è dimostrabile logicamente?\n",
    "\n",
    "### Due scenari\n",
    "#### 1. Il problema della nonna\n",
    "Esistono problemi per i quali **non siamo in grado di formulare una soluzione**, anche perché può non esistere.  \n",
    "Per una persona, stabilire se una foto ritragga o meno sua nonna è un compito triviale; per un computer non lo è affatto. Questo è un esempio di problema \"_non risolvibile_\", per cui ricorriamo al ML.\n",
    "\n",
    "#### 2. Il problema dello zaino\n",
    "Per alcuni problemi, formulare una soluzione è del tutto fattibile (se non facile), ma questa ha un **costo computazionale proibitivo**.  \n",
    "Nel <a href=\"https://it.wikipedia.org/wiki/Problema_dello_zaino\">problema dello zaino</a>, la soluzione _brute force_ è concettualmente semplice: basta considerare tutte le possibili combinazioni di oggetti e scegliere quella con il valore massimo senza superare il peso limite. Tuttavia, questa strategia diventa impraticabile al crescere del numero di oggetti ($n$): ad esempio, con $n=50$, si arriva a oltre un milione di miliardi di combinazioni da esaminare ($2^{50}$) .\n",
    "\n",
    "In queste due situazioni ricorriamo ad un modello di ML, nell'ottica che le sue risposte approssimino bene quelle esatte, se il problema ne prevede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7443356-0d79-490e-821c-06ae2f0e06b2",
   "metadata": {},
   "source": [
    "## Modalità di apprendimento\n",
    "In base a come si presentano i dati che l’algoritmo di apprendimento usa per produrre il modello, distinguiamo tra apprendimento **supervisionato** e non.\n",
    "\n",
    "### Apprendimento supervisionato\n",
    "Ogni osservazione è rappresentata da una coppia $(x, y)$, dove $x$ è l'istanza del problema e $y$ è la relativa soluzione (o etichetta).  \n",
    "Ad esempio, se $x$ è una foto della nonna da classificare, $y$ sarà la risposta _sì/no_.  \n",
    "Il dataset è quindi un insieme di coppie _istanza-soluzione_: $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}$.  \n",
    "Chiaramente deve esistere una relazione $f$ che leghi istanze e soluzioni, tale che $f(x) = y$. L'obiettivo del modello è proprio approssimare $f$, che prende il nome di _ground truth_. Le predizioni del modello vengono indicate come $\\hat{y}$.\n",
    "\n",
    "### Apprendimento non supervisionato\n",
    "Ogni osservazione è costituita esclusivamente dall'istanza del problema: $(x)$.  \n",
    "Un esempio di apprendimento non supervisionato è il _clustering_, in cui il modello deve raggruppare i dati in insiemi (_cluster_) basati su somiglianze o caratteristiche comuni. L'obiettivo è che le osservazioni all'interno di ciascun gruppo siano più simili tra loro rispetto a quelle di altri gruppi, permettendo di individuare pattern e strutture nascoste nei dati senza l'utilizzo delle etichette.\n",
    "\n",
    "**_Nota_**: il notebook si concentra sul ML supervisionato."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dbb0c48-04ce-448a-b934-6c030850c72e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Valutare un modello\n",
    "Poiché l'obiettivo è approssimare la relazione di base (_ground truth_) tra istanze e soluzioni, è necessario **valutare la bontà di tale approssimazione**, ossia valutare il modello.  \n",
    "L'idea di base è molto semplice: si forniscono al modello delle osservazioni (coppie $(x, y)$), e per ciascuna si confronta la risposta prodotta dal modello, $\\hat{y}$, con la soluzione reale, $y$.  \n",
    "A partire da questo confronto, è possibile monitorare diverse **metriche** di valutazione delle prestazioni.\n",
    "\n",
    "### Alcune metriche\n",
    "Presentiamo alcune delle metriche che vengono comunemente impiegate per la valutazione delle prestazioni.  \n",
    "Notazione:\n",
    "- $D$ : dataset, composto da esempi del tipo $(x_i, y_i)$\n",
    "- $m = |D|$\n",
    "- $f$ : modello; $f(x_i)$ è l'output del modello quando sottoposto all'input (istanza) $x_i$\n",
    "- $\\mathbb{I}$ : <a href=\"https://it.wikipedia.org/wiki/Funzione_indicatrice\">funzione indicatrice</a>\n",
    "\n",
    "#### MSE\n",
    "L'errore quadratico medio (Mean Squared Error) è tipicamente utilizzato nei problemi di regressione (come quello descritto nel paragrafo successivo).\n",
    "$$\n",
    "MSE(f;D)= \\frac{1}{m}\\sum_{i=1}^{m} (f(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "#### Tasso d'errore (_error rate_) e accuratezza (_accuracy_)\n",
    "Sono le due metriche più diffuse nei problemi di classificazione.  \n",
    "Il tasso d'errore indica la proporzione di istanze erroneamente classificate sul totale delle istanze.\n",
    "$$\n",
    "E(f;D)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{I}(f(x_i)\\neq y_i)\n",
    "$$\n",
    "\n",
    "L'accuratezza indica la proporzione di istanze correttamente classificate sul totale delle istanze.\n",
    "$$\n",
    "acc(f;D)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{I}(f(x_i)=y_i)\n",
    "$$\n",
    "Valgono anche:\n",
    "$$\n",
    "acc(f;D)=1-E(f;D)\n",
    "$$\n",
    "$$\n",
    "E(f;D)=1-acc(f;D)\n",
    "$$\n",
    "\n",
    "#### Precisione (_precision_) e richiamo (_recall_)\n",
    "Nel sottocaso dei problemi di classificazione binaria, si tratta di due metriche molto comuni e più informative rispetto a tasso d'errore o accuratezza.  \n",
    "La precisione indica, in rapporto, quanti dei campioni previsti come positivi sono effettivamente positivi.\n",
    "$$\n",
    "\\frac{Vero\\, positivi\\, (TP)}{Vero\\, positivi\\, (TP) + Falsi\\, positivi\\, (FP)}\n",
    "$$\n",
    "La si può vedere come una misura dell'accuratezza delle previsioni positive.\n",
    "\n",
    "Il richiamo indica la proporzione di esempi positivi reali che sono stati correttamente identificati dal modello.\n",
    "$$\n",
    "\\frac{Vero\\, positivi\\, (TP)}{Vero\\, positivi\\, (TP) + Falsi\\, negativi\\, (FN)}\n",
    "$$\n",
    "Lo si può vedere come una misura della capacità del modello di trovare tutti i positivi.  \n",
    "\n",
    "Importante notare come precisione e richiamo siano in mutua \"competizione\". In generale, il richiamo è spesso basso quando la precisione è alta, e la precisione è spesso bassa quando il richiamo è alto.  \n",
    "Infatti, se volessimo facilmente portare a 1 il richiamo del nostro modello, basterebbe far sì che questo classifichi positivamente ogni istanza, portando logicamente a 0 i falsi negativi. Ma questo andrebbe a scapito della precisione, per via dei numerosi ed inevitabili falsi positivi. Al contrario, classificando come positive solo le istanze di cui si è quasi certi, dunque massimizzando la precisione, si genererebbero parecchi falsi negativi che inciderebbero negativamente sul richiamo.  \n",
    "Tipicamente, possiamo ottenere alti livelli per entrambe le metriche solo nei problemi più semplici. In quelli complessi, dobbiamo capire quale metrica è più importante.\n",
    "Ecco due esempi significativi:\n",
    "- In un sistema di filtraggio dello spam, è fondamentale che le email classificate come \"spam\" siano effettivamente spam. Un'alta precisione è importante perché non vogliamo che email legittime (non spam) vengano erroneamente contrassegnate come spam (falsi positivi). In questo caso, se il modello ha bassa precisione, potremmo perdere messaggi importanti.\n",
    "- In un sistema che rileva una malattia pericolosa, il richiamo è più importante perché è cruciale individuare tutti i pazienti malati, anche a costo di avere alcuni falsi positivi. Se il modello ha un basso richiamo, rischieremmo di non diagnosticare correttamente alcune persone malate (falsi negativi).\n",
    "\n",
    "### Con quali dati?\n",
    "La scelta dei dati da utilizzare per valutare le performance del modello è un aspetto a cui prestare attenzione. Intuitivamente, vorremmo utilizzare gli stessi dati che abbiamo usato per allenare il modello, ossia quelli appartenenti al _**train set**_ $S$.\n",
    "Vediamo come mai questa sia in realtà una cattiva idea.\n",
    "\n",
    "Prendiamo in esame un semplice problema di regressione. Ogni esempio è costituito da due valori reali, $x$ (l'istanza) e $y$ (l'etichetta) e dunque individua un punto nel piano. I punti neri costituiscono $S$, mentre quelli bianchi corrispondono a dati nuovi, mai visti dal modello, e quindi ancora da etichettare. L'obiettivo è, come sempre, approssimare la ground truth, e questo problema permette di visualizzarlo facilmente. Di fatto, in questo caso, il modello non è altro che una curva, e lo valuteremo in base a quanto si discosta mediamente dai punti del dataset (MSE).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/regressione.jpeg\" alt=\"Descrizione\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "Chiaramente il modello ideale sarebbe $A$, che interpola bene tutti i punti, sia quelli incontrati in $S$, sia quelli nuovi. Di certo non potremmo dire che $B$ sia un buon modello: si discosta notevolmente dai dati non incontrati. Eppure, se lo valutassimo solo su $T$, performerebbe come $A$ (se non meglio). $B$ soffre del cosiddetto problema di **overfitting**: si è specializzato solo sul train set, non ha colto la relazione sottostante ai dati e dunque non è in grado di generalizzare su dati mai visti.  \n",
    "Pertanto, **è fondamentale testare il modello su un _test set_ $T$**, cioè un insieme di dati disgiunto da $S$.\n",
    "\n",
    "Questo non significa che valutare un modello sul train set sia inutile: performance scarse sul train set indicano che il modello utilizzato non è abbastanza espressivo/potente per il dato problema (fenomeno noto come **underfitting**), e dunque costituiscono una prima informazione preziosa. L’importante è riconoscere che performance buone sul train set non implicano qualità del modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce266ef-68f6-4c6b-9617-8dba860f7b00",
   "metadata": {},
   "source": [
    "## Due tipologie di modello\n",
    "Presentiamo molto brevemente due tra le numerose tipologie di modello di ML, anche per illustrare più facilmente alcuni concetti più avanti.\n",
    "\n",
    "### Albero di decisione\n",
    "Si tratta di un albero in cui ogni nodo interno rappresenta una decisione basata su una caratteristica (o attributo) dell'esempio, e le foglie rappresentano le classi o le previsioni finali. Fissata un’altezza massima, l’algoritmo di apprendimento produce, partendo dai dati, un albero di decisione; deve quindi stabilirne la struttura, le condizioni presenti nei nodi interni e le classi nelle foglie.  \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/decision_tree.png\" alt=\"Descrizione\" width=\"400\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "### KNN\n",
    "Gli esempi vengono disposti in uno spazio avente una dimensione per ogni possibile caratteristica/attributo delle istanze. Dunque ogni esempio corrisponde ad un punto in questo spazio.  \n",
    "Per classificare una nuova istanza, rappresentata da un nuovo punto, KNN calcola la distanza tra quest'ultimo e tutti i punti del dataset (solitamente utilizzando la distanza euclidea).\n",
    "Vengono poi selezionati i $k$ punti più vicini (da qui \"$k$-nearest neighbors\").\n",
    "La classe del nuovo punto viene determinata dalla classe più frequente tra i $k$ vicini selezionati nel caso della classificazione, oppure dalla media dei valori nel caso della regressione.  \n",
    "Nel caso di KNN non c’è una vera e propria fase di allenamento (vedremo poi nel paragrafo dedicato a parametri e iperparametri); semplicemente i dati vengono memorizzati.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/knn.png\" alt=\"Descrizione\" width=\"300\" height=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235e47a-763d-4838-9ad8-62950f504cfe",
   "metadata": {},
   "source": [
    "## Parametri e iperparametri\n",
    "I parametri sono le caratteristiche del modello che vengono **definite nella fase di apprendimento**. Gli iperparametri, invece, sono le caratteristiche del modello che devono essere fissate __*prima* della fase di apprendimento__.\n",
    "\n",
    "Per gli alberi di decisione, ha senso stabilire come prima cosa un’altezza massima, che costituisce quindi un iperparametro. Fissata quella, l’allenamento stabilisce la topologia, le decisioni presenti nei nodi interni e le classi delle foglie; che sono i parametri. **Lo scopo dell'allenamento è trovare i valori ottimali per i parametri del modello**.  \n",
    "Prima dell'allenamento, i parametri hanno valori di default o addirittura casuali; al termine dell'allenamento, i parametri hanno i valori ottimali, ossia quelli che consentono al modello di compiere predizioni accurate.  \n",
    "Nel caso di KNN, non c’è una vera e propria fase di apprendimento; di fatto non ci sono parametri da stabilire. C’è solo un iperparametro, ossia $k$.\n",
    "\n",
    "Ma se l’algoritmo di apprendimento trova i parametri, come si trovano gli iperparametri?\n",
    "\n",
    "### Tuning degli iperparametri\n",
    "In questa sezione prendiamo come riferimento KNN, che ha un solo iperparametro: $k$. Sappiamo che non c’è un vero e proprio allenamento, ma lo includeremo tra le fasi, generalizzando (si può pensare ad una fase di “collocazione dei dati nello spazio”).\n",
    "\n",
    "Non c’è un modo particolarmente furbo di trovare il valore ottimale di $k$. La cosa migliore da fare è anche la più intuitiva: stabilire un insieme di $n$ possibili valori per $k$ e lanciare un algoritmo di apprendimento per ciascuno di questi valori, generando così $n$ modelli allenati sullo stesso train set $S$. Ciascun modello viene poi valutato e si sceglie quello con la performance migliore.  \n",
    "Come già discusso [qua](#Valutare-un-modello), non possiamo misurare le performance limitandoci al $S$. Dunque, per scegliere il modello migliore tra gli $n$ generati, ricorriamo ad un cosiddetto **validation set** $V$, disgiunto da $S$. Denotiamo come $k^*$ l'iperparametro del modello che performa meglio.  \n",
    "Viene poi generato un nuovo modello $m$ avente $k = k^*$, ma allenandolo sull'unione $S \\cup V$. Chiaramente più dati si danno in pasto al modello, meglio performerà.  \n",
    "Infine, si utilizza un apposito test set $T$ per valutare le performance di $m$. Se sono soddisfacenti, si fa un altro “giro” di allenamento, questa volta su $S \\cup V \\cup T$.\n",
    "\n",
    "Se si hanno due o più iperparametri, si procede analogamente, testando ogni loro possibile combinazione e scegliendo quella che dà luogo alla performance migliore su $V$. Chiaramente le risorse computazionali richieste aumentano non poco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c328f0-41a5-4cc4-8ab7-eb2f2924af4b",
   "metadata": {},
   "source": [
    "## Gestire il dataset\n",
    "Dato il dataset $ D = \\{(x_1, y_1), \\dots, (x_n, y_n)\\} $, come possiamo ricavarci un train set $S$, un test set $T$ e, se necessario, un validation set $V$?\n",
    "\n",
    "Di seguito illustreremo alcune tecniche che consentono di farlo, insieme al concetto di campionamento stratificato, fondamentale per la buona riuscita dell'allenamento e della valutazione di un modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f71ae8-68fa-42b2-acb7-b551cb01ec68",
   "metadata": {},
   "source": [
    "### Campionamento stratificato\n",
    "Un dataset non è altro che un campione della popolazione, dunque deve rifletterne la distribuzione. È importante che tale distribuzione sia preservata anche in qualsiasi altro sottoinsieme di $D$ ($S$, $T$ e $V$). Questo lo scopo delle tecniche di _stratified sampling_ (campionamento stratificato).  \n",
    "Se $S$ e $T$ avessero distribuzioni diverse, vorrebbe dire testare il modello su una popolazione distribuita diversamente rispetto a quella su cui si è allenato.\n",
    "\n",
    "Prendiamo come esempio un problema di classificazione binaria. Supponiamo di avere $D$ contenente 500 esempi positivi e 500 esempi negativi, che vogliamo suddividere facendo holdout (link al paragrafo) in un train set $S$ con il 70% degli esempi e un test set $T$ con il 30% degli esempi. In tal caso, un metodo di campionamento stratificato garantirà che $S$ contenga 350 esempi positivi e 350 esempi negativi, e che $T$ contenga 150 esempi positivi e 150 esempi negativi.\n",
    "\n",
    "Da qua in poi assumeremo sempre che qualsiasi sottoinsieme ricavato da $D$ sia correttamente stratificato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670be22-07a9-44f9-95cb-1e5d64a9cb52",
   "metadata": {},
   "source": [
    "### Holdout\n",
    "\n",
    "La metodologia Hold Out consiste nel suddividere $ D $ in nel train set $ S $ e nel test set $ T $, in modo tale che $ D = T \\cup S $ e $ T \\cap S = \\emptyset $.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/holdout.webp\" alt=\"Descrizione\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "La domanda che sorge è: quanti esempi inserire in $S$? Quanti in $T$? \n",
    "Se insieriamo quasi tutti gli esempi in $S$, allora il modello che otteniamo è un'ottima approssimazione del modello allenato su $D$, il che è desiderabile poiché, generamente, più dati fornisco all'algoritmo di apprendimento, migliore è il modello che ottengo. Tuttavia, la valutazione delle performance è meno affidabile a causa delle dimensioni ridotte di $T$. D'altra parte, se inseriamo più esempi in $T$, la differenza tra il modello addestrato su $S$ e quello addestrato su $D$ diventa eccessiva. Non esiste una soluzione perfetta a questo dilemma; è necessario un compromesso. Una pratica comune è utilizzare dai 2/3 ai 4/5 degli esempi in $D$ per l'addestramento e il resto per il test.\n",
    "\n",
    "#### Holdout ripetuto\n",
    "Poniamo di voler ricavare solo $S$ e $T$. Mantenendo costante il numero di esempi in $S$ (e quindi il numero di esempi in $T$), a partire dallo stesso $D$ si possono ottenere più coppie $(S,T)$. Di fatto, nella pratica, si eseguono $n$ holdout (ad es. 100). Dunque alleno e valuto $n$ modelli, ciascuno su una coppia $(S,T)$ diversa, sempre ottenuta da $D$.\n",
    "Come valutazione complessiva, si prende la media delle $n$ valutazioni eseguite, e si allena un modello sull’intero $D$, che sarà poi il modello definitivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777480c-f3b4-429b-aec5-4c878291a92e",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "La metodologia cross-validation prevede di suddividere $ D $ in $ k $ sottoinsiemi di cardinalità uguale o simile, tali che $ D = \\bigcup_{i=1}^{k} D_i $ e $ \\forall i, j \\in \\{1, \\dots, k\\}, D_i \\cap D_j = \\emptyset $, con $ i \\neq j $. In altre parole, i $k$ sottoinsiemi costituiscono una partizione di $D$.  \n",
    "Vengono svolte $ k $ iterazioni; a ogni iterazione, un singolo insieme $ D_i $ non ancora utilizzato come test set, viene scelto come tale, mentre il training set viene composto dall'unione di tutti gli altri sottoinsiemi. A questo punto, possiamo ottenere la valutazione del modello.  \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/crossvalidation.png\" alt=\"Descrizione\" width=\"500\" height=\"500\">\n",
    "</div>\n",
    "Finite le iterazioni, disponiamo di $ k $ valutazioni, da cui otteniamo un'unica valutazione complessiva calcolando la media.  \n",
    "Poiché la stabilità e l'accuratezza della cross-validation dipendono in gran parte dal valore di $ k $, essa è anche conosciuta come k-fold cross-validation. Il valore di $ k $ più comunemente utilizzato è 10, e il metodo corrispondente è chiamato 10-fold cross-validation.\n",
    "\n",
    "Come nel metodo holdout, ci sono diversi modi per suddividere il dataset $ D $ in $ k $ sottoinsiemi. Per ridurre l'errore introdotto dalla suddivisione, spesso ripetiamo la suddivisione casuale $ p $ volte e facciamo la media dei risultati delle $ p $ iterazioni di k-fold cross-validation. Un caso comune è la 10-times 10-fold cross-validation.\n",
    "\n",
    "Un caso speciale di cross-validation è il **Leave-One-Out** (LOO), in cui abbiamo un dataset con cardinalità $ m $ e impostiamo $ k = m $. Le valutazioni sono molto accurate, ma il costo computazionale dell'addestramento di $ m $ modelli può essere proibitivo per dataset di grandi dimensioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab651ef0-e56b-4777-886e-a5e9c09b14c9",
   "metadata": {},
   "source": [
    "### Ricavare $V$\n",
    "\n",
    "Finora abbiamo visto come si possono ricavare $S$ e $T$ da $D$, ma non abbiamo discusso il caso in cui si debba anche fare tuning degli iperparametri, ricorrendo quindi al validation set nella modalità descritta nel paragrafo appena citato.   \n",
    "In questo caso, le suddivisioni vengono annidate: abbiamo una suddivisione esterna, da $D$ a $S$ e $T$, e una interna, da $S$ a $S$ ridotto e $V$.  \n",
    "Le due suddivisioni possono essere eseguite con lo stesso metodo (ad esempio holdout-holdout), oppure con metodi differenti (ad esempio holdout-cv). L'immagine visualizza una suddivisione annidata holdout-cv.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/tuning.png\" alt=\"Descrizione\" width=\"400\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "Merita particolare attenzione il caso in cui si scelga di utilizzare la cross-validation per la suddivisione esterna. A ogni iterazione esterna corrisponde un train set diverso, dunque un modello allenato diversamente, i cui iperparametri ottimali possono quindi differire da quelli dei modelli relativi alle altre iterazioni esterne. Ogni iterazione esterna può potenzialmente produrre un modello \"unico\" nei suoi iperparametri. In questo caso, possiamo scegliere uno qualsiasi dei modelli addestrati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913c8b3-460e-4b08-87df-d407e6d02215",
   "metadata": {},
   "source": [
    "## Esempi pratici usando scikit-learn\n",
    "Vediamo degli esempi pratici che coinvolgono KNN. Utilizziamo il **dataset _iris_**, già disponibile in scikit-learn, che comprende osservazioni relative a tre sottospecie di pianta iris: _Setosa_, _Versicolor_ e _Virginica_.  \n",
    "Affrontiamo un semplice problema di classificazione binaria: dati i quattro attributi del dataset, dobbiamo stabilire se una pianta appartenga o meno alla sottospecie Setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c83bfaaf-6ef6-4359-a006-1fbba6033a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4321e-86ab-4f27-80d6-2adca82ead99",
   "metadata": {},
   "source": [
    "I quattro attributi che compongono ciascuna istanza si riferiscono a lunghezza e larghezza dei sepali e dei petali delle piante osservate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c43cbbba-b7a6-4788-b630-60729e2976d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55c707-53f9-40f5-a17b-0584f4cbcc86",
   "metadata": {},
   "source": [
    "Modifichiamo il dataset perché il target sia di tipo binario. Dobbiamo distinguere solo tra Setosa e non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77f68a54-43f8-4f8b-9293-02550ec459a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target'] = np.where(iris['target'] == 2, 1, iris['target'])\n",
    "iris['target_names'] = np.array(['setosa', 'versicolor/virginica'])\n",
    "iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa505d5d-c957-4925-80b7-5ac58f065686",
   "metadata": {},
   "source": [
    "Ora che il dataset è pronto, passiamo alla fase di training. Vogliamo anche fare il tuning dell'iperparametro $k$. Come già discusso, possiamo adottare diversi metodi per suddividere il dataset negli insiemi necessari. Di seguito, vediamo sia un approccio holdout-holdout, che un approccio holdout-cv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc1e6f-6292-4190-8447-620e7a04207a",
   "metadata": {},
   "source": [
    "### Approccio holdout-holdout\n",
    "Eseguiamo il primo holdout per ottenere train set e test set, utilizzando il metodo train_test_split di scikit-learn. Attraverso il parametro test_size, impostiamo la percentuale di esempi da includere nel test set (e quindi quella da includere nel train set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d238cdb-8560-414f-9210-1c103a6530af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_training_set, X_test_set, y_training_set, y_test_set = train_test_split(\n",
    "    iris['data'], \n",
    "    iris['target'], \n",
    "    test_size=0.3,  \n",
    "    stratify=iris['target']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d800e5e-a76b-4bfc-9159-acf8504058ca",
   "metadata": {},
   "source": [
    "Ripetiamo l'holdout per ottenere, a partire dal train set appena ricavato, un train set ridotto e un validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e23b7c04-0fd4-434f-8b80-4f3132776b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set, X_validation_set, y_train_set, y_validation_set = train_test_split(\n",
    "    X_training_set,\n",
    "    y_training_set,\n",
    "    test_size=0.3,\n",
    "    stratify=y_training_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb74e8f-b69c-4125-a437-e70b2ac10ac6",
   "metadata": {},
   "source": [
    "Ora diamo il via ad una classica ricerca del massimo. Inizializziamo il modello KNN con $k=1$, lo alleniamo e ne calcoliamo l'accuratezza sul validation set. Dopodiché, ripetiamo per diversi valori di $k$, cercando quello che massimizza l'accuratezza.  \n",
    "Al termine del ciclo, la variabile best_model conterrà il modello avente il $k$ ottimale.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba0c50e5-c262-4917-b31a-3ae29a7777ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = KNeighborsClassifier(n_neighbors=1)\n",
    "best_model.fit(X_train_set, y_train_set)\n",
    "accuracy = accuracy_score(y_validation_set, best_model.predict(X_validation_set))\n",
    "        \n",
    "for k in range(3, 10, 2):\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train_set, y_train_set)\n",
    "    accuracy_temp = accuracy_score(y_validation_set, model.predict(X_validation_set))\n",
    "    if(accuracy_temp > accuracy):\n",
    "        best_model = model\n",
    "        accuracy = accuracy_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e30e4-ca4f-4c23-81b0-c33805a0fb56",
   "metadata": {},
   "source": [
    "Calcoliamo infine l'accuratezza del modello migliore sul test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfe7c033-ab0b-4f59-bfa4-b7f2437c4a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_set, best_model.predict(X_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae2154-1122-4643-8aa3-aed82c0de779",
   "metadata": {},
   "source": [
    "### Approccio holdout-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d23b79-281c-4132-a0f3-b726f52aed49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_training_set, X_test_set, y_training_set, y_test_set = train_test_split(\n",
    "    iris['data'], \n",
    "    iris['target'], \n",
    "    test_size=0.3,  \n",
    "    stratify=iris['target']\n",
    ")\n",
    "    \n",
    "model = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
    "gs = GridSearchCV(model, parameters, cv=10)\n",
    "gs.fit(X_training_set, y_training_set)\n",
    "accuracy_score(y_test_set, gs.predict(X_test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
